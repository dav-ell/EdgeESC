{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "The purpose of this code is to create a TensorFlow Lite model. To do this, we supply a data set, create a TensorFlow model, train the model on that data set, then convert the TF model into a TF Lite model. This TF Lite model can then be loaded onto a mobile device for audio classification. \n",
    "\n",
    "There are a variety of models that can be trained on a variety of data sets. This code is for training MFCC models, which which takes a couple of extra steps in data processing compared to its amplitude counterpart. This guide aims to describe all of the parameters in the training process, so that you can change them and build your own models. Remember, the end goal is to acheive the highest validation accuracy possible before loading that model onto a phone for use.\n",
    "\n",
    "**Numbers to beat for our l6-data:** <br>\n",
    "*A good goal--* 85% val_acc <br>\n",
    "*Our best model--* 94% val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "We use Keras(which is built on top of Tensorflow) to build and train our models. Librosa is used for audio processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure kernel matches pip version\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "# Keras\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Activation, Dense, Dropout, Flatten, Conv2D, Conv1D, \n",
    "                          MaxPooling2D, GlobalAveragePooling2D, MaxPooling1D, Lambda)\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.utils import to_categorical, multi_gpu_model\n",
    "import keras.backend as K\n",
    "\n",
    "import librosa\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "from scipy.fftpack import dct\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm as tqdm\n",
    "import time\n",
    "from pprint import pprint\n",
    "import uuid\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "\n",
    "## General\n",
    "\n",
    "**RAW_DATA_DIR:** Where the raw training data is located. In the directory each folder name is a label, and its contents are .wav files corresponding to that label. See the \"l6-data\" directory for an example. \n",
    "\n",
    "**MFCC_PROCESSED_DATA_DIR:** Where the processed training data is to be stored. After processing the data, it will be populated with numpy files. Each numpy file is named after a label, and contains all of the training data for that label stored as a 4d numpy array. More info on this later.\n",
    "\n",
    "**AUDIO_LENGTH:** The desired input size for the model. An input size of 44100 at 44100 Hz would be a one second input. \n",
    "\n",
    "**SAMPLE_RATE:** The sample rate of the microphone.\n",
    "\n",
    "## MFCC\n",
    "\n",
    "**nmfcc:** The number of mfccs to use. This directly affects the shape of the input to your model. \n",
    "\n",
    "**nmels:** A pre-computed log-power Mel spectrogram. Not sure what this number does exactly, but everyone seems to only ever use 128. Librosa documentation isn't very thorough.\n",
    "\n",
    "**hop_length:** The hop length for calculating MFCCs.\n",
    "\n",
    "**n_fft:** Number of FFTs for calculating MFCCs.\n",
    "\n",
    "**nframe:** The number of frames for MFCCs. This directly affects the shape of the input to your model, and is based on the audio length and hop length. Makes sense if you think about it. \n",
    "\n",
    "## Training\n",
    "\n",
    "**channel:** We use a one channel audio input\n",
    "\n",
    "**epochs:** This is how many times the model will train on your data. 200 is usually a good number, there are diminishing returns after a certain point. \n",
    "\n",
    "**batch_size:** I don't even know what this is.\n",
    "\n",
    "**verbose:** 1 is true, 0 is false. Its always a good idea to have this on. \n",
    "\n",
    "**num_classes:** The number of classes (also referred to as labels) in your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = \"data/\"\n",
    "MFCC_PROCESSED_DATA_DIR = \"mfcc-processed-data/\"\n",
    "AUDIO_LENGTH = 44100\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "nmfcc = 128\n",
    "nmels = 128\n",
    "hop_length = 512\n",
    "n_fft = 1024\n",
    "nframe = int(math.ceil(AUDIO_LENGTH / hop_length))\n",
    "print(nframe)\n",
    "\n",
    "channel = 1\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "verbose = 1\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "### get_labels(path)\n",
    "**Input:** `RAW_DATA_DIR` <br>\n",
    "**Output:** `Tuple (Labels, Indices of the labels, one-hot encoded labels)` <br>\n",
    "**Description:** Gets all labels (aka filenames) inside your raw data directory. The indices are the positions of these labels, and the hot encoded vector is a vector of zeros the length of the number of labels, with a 1 at the corresponding label's index. More info can be found about these online.\n",
    "\n",
    "### wav2mfcc(file_path)\n",
    "**Input:** `file` - .wav file <br>\n",
    "**Output:** `mfcc_vectors` - array with shape: (nmfccs, nframes) <br>\n",
    "**Description:** Takes a .wav file and converts it to MFCCs using librosa and the MFCC constants declared above.\n",
    "\n",
    "### label_to_mfcc_vecs(args)\n",
    "**Input:** `Tuple (label, input_path, output_path, tqdm_position)`<br>\n",
    "**Output:** Shape of the generated numpy file (stored under `output_path`)<br>\n",
    "**Description:** This function is called by `process_data_mfcc()` in parallel to convert a label's raw .wav files to a single numpy file. This can take a while, so David made a super fancy tdqm display (hence the tqdm_position parameter). **This function will convert the training data into overlapping MFCCs with dimensions (nmfcc, nframe), then store them in a 4d numpy array under the `output_path` dir.** It will move though the clip at a rate of AUDIO_LENGTH / 2, so if a clip is 10.1 seconds long, and the input length is 44100 with a 44100 Hz sampling rate, it will be processed as 21 one second clips. The final piece of the clip is padded with zeros to match the input length. This is done for all the .wav files in a label, which are converted to amplitude arrays with `librosa.load()`, then mfccs with `librosa.feature.mfcc()`. The resulting numpy array contains the MFCCs that are actually trained on.\n",
    "\n",
    "### process_data_mfcc(input_path, output_path)\n",
    "**Input:** RAW_DATA_DIR, AMP_PROCESSED_DATA_DIR<br>\n",
    "**Output:** Shape of the numpy files stored in output_path<br>\n",
    "**Description:** Calls ` label_to_mfcc_vecs()`, and populates output_path with numpy files. Each numpy file is named after a label, and its content is a 4d numpy array with all the training data for that label. The shape of this array is (number_of_files, nmfcc, nframes, 1). The 4th empty dimension is required by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path):\n",
    "    labels = [i for i in sorted(os.listdir(path)) if i[0] != \".\"]\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)\n",
    "\n",
    "def wav2mfcc(file_path):\n",
    "    mfcc_vectors = []\n",
    "    audio_buf,_ = librosa.load(file_path, mono=True, sr=SAMPLE_RATE)\n",
    "    audio_buf = (audio_buf - np.mean(audio_buf)) / np.std(audio_buf)\n",
    "    \n",
    "    remaining_buf = audio_buf.copy()\n",
    "    while remaining_buf.shape[0] > AUDIO_LENGTH:\n",
    "        # Add the first AUDIO_LENGTH of the buffer as a new vector to train on\n",
    "        new_buf = remaining_buf[ : AUDIO_LENGTH ]\n",
    "        mfcc = librosa.feature.mfcc(new_buf, sr=SAMPLE_RATE,S=None, n_mfcc=nmfcc, n_fft=n_fft, hop_length=hop_length, n_mels=nmels)\n",
    "        mfcc_vectors.append(mfcc)\n",
    "\n",
    "        # Remove 1/2 * AUDIO_LENGTH from the front of the buffer\n",
    "        remaining_buf = remaining_buf[ int(AUDIO_LENGTH / 2) : ]\n",
    "\n",
    "    # Whatever is left, pad and stick in the training data\n",
    "    remaining_buf = np.concatenate((remaining_buf, np.zeros(shape=(AUDIO_LENGTH - len(remaining_buf)))))\n",
    "    mfcc = librosa.feature.mfcc(remaining_buf,sr=SAMPLE_RATE,S=None, n_mfcc=nmfcc, n_fft=n_fft, hop_length=hop_length,n_mels=nmels)\n",
    "    mfcc_vectors.append(mfcc)\n",
    "    \n",
    "    return mfcc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_mfcc_vecs(args) -> None:\n",
    "    label, input_path, output_path, tqdm_position = args\n",
    "    \n",
    "    vectors = []\n",
    "\n",
    "    wavfiles = [os.path.join(input_path, label, wavfile) for wavfile in os.listdir(os.path.join(input_path, label))]\n",
    "    \n",
    "    # tqdm is amazing, so print all the things this way\n",
    "    print(\" \", end=\"\", flush=True)\n",
    "    twavs = tqdm(wavfiles, position=tqdm_position)\n",
    "    for i, wavfile in enumerate(twavs):\n",
    "        vectors_for_file = wav2mfcc(wavfile)\n",
    "        for v in vectors_for_file:\n",
    "            vectors.append(v)        \n",
    "        # Update tqdm\n",
    "        twavs.set_description(\"Label - '{}'\".format(label))\n",
    "        twavs.refresh()\n",
    "#     np.delete(vectors, 0)  # deletes first zero entry    \n",
    "    np_vectors = np.array(vectors)\n",
    "    np.save(os.path.join(output_path, label + '.npy'), np_vectors)\n",
    "    return np_vectors.shape   \n",
    "\n",
    "def process_data_mfcc(input_path, output_path):\n",
    "    if not os.path.exists(MFCC_PROCESSED_DATA_DIR):\n",
    "        os.mkdir(MFCC_PROCESSED_DATA_DIR)\n",
    "    \n",
    "    labels, _, _ = get_labels(input_path)\n",
    "    pool = mp.Pool()\n",
    "    result = pool.map(label_to_mfcc_vecs, \n",
    "                     [(label, input_path, output_path, tqdm_position) \n",
    "                          for tqdm_position, label in enumerate(labels)])\n",
    "    pool.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data_mfcc(RAW_DATA_DIR, MFCC_PROCESSED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "### get_train_test(split_ratio=0.75, random_state=42)\n",
    "**Inputs:** `split_ratio` <br>\n",
    "**Outputs:** `X_train, X_test, y_train, y_test` <br>\n",
    "**Description:** Uses a sklearn library to split the processed data into training data and test data. We use a .75 ratio of training to test data. X_train is the training data, which is a 4d array with dimnesions (number_of_files, nmfcc, nframes, 1). Again, the 4th dimension is a requirement from Keras. y_train is a 1d array containing label indices that correspond to each clip in X_train. X_test and y_test are the same, but for testing data instead of training data. The testing data is used to calculate accuracy metrics during training.\n",
    "### get_model()\n",
    "**Inputs:** None <br>\n",
    "**Outputs:** `Tensorflow Model` <br> \n",
    "**Description:** This function is where the model itself is built. The input size must match the clip length (e.g. our MFCC shapes), and the output must be a set of weights corresponding to each label (so an array of length labels). The highest weight will correspond to the final classification. The models we use are CNNs, which are built layer by layer. Any model with the proper input and output sizes can be loaded. In our case, the input shape was (nmfcc, nframes, 1), as Keras required the extra dimension. \n",
    "\n",
    "### train(model, X_train, y_train_hot, X_test, y_test_hot)\n",
    "**Inputs:** `model, X_train, y_train_hot, X_test, y_test_hot`<br>\n",
    "**Outputs:** The given model is trained on the given data. <br>\n",
    "**Description:** We use 'adam' optimization during training, as it usually results in much higher accuracies. We also save the model at its highest validation accuracies as it trains, so that we can retrieve the model from the epoch with the highest accuracy. These are saved as \"weights-{val_acc}.hdf5\", and can be converted to TF Lite models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(split_ratio=0.75, random_state=42):\n",
    "    # Get available labels\n",
    "    labels, indices, _ = get_labels(RAW_DATA_DIR)\n",
    "\n",
    "    # Getting first arrays\n",
    "    X = np.load(os.path.join(MFCC_PROCESSED_DATA_DIR, labels[0] + '.npy'))\n",
    "    y = np.zeros(X.shape[0])\n",
    "\n",
    "    # Append all of the dataset into one single array, same goes for y\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        x = np.load(os.path.join(MFCC_PROCESSED_DATA_DIR, label + '.npy'))\n",
    "#         print(label)\n",
    "#         print(x.shape)\n",
    "#         print(X.shape)\n",
    "        X = np.vstack((X, x))\n",
    "        y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
    "\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading train set and test set\n",
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Reshaping to perform 2D convolution\n",
    "print(X_train.shape)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], nmfcc, nframe, channel)\n",
    "X_test = X_test.reshape(X_test.shape[0], nmfcc, nframe, channel)\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train_hot.shape)\n",
    "print(y_test_hot.shape)\n",
    "print(X_train[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(16, \n",
    "                     kernel_size=3, padding='same', activation='relu',name='voice', input_shape=(nmfcc,nframe, channel)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    model.add(Conv2D(32,kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    model.add(Conv2D(64,kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    model.add(Conv2D(128,kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    model.add(Conv2D(256,kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv2D(512,kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    model.add(Conv2D(1024,kernel_size=2,padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(num_classes, kernel_size=1,padding='same', activation='sigmoid'))\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train_hot, X_test, y_test_hot):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=10, min_lr=0.0001, verbose=1)\n",
    "    mcp_save = ModelCheckpoint('out/weights.{epoch:02d}-{val_acc:.2f}.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "    model.fit(X_train, \n",
    "              y_train_hot, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              verbose=verbose, \n",
    "              validation_data=(X_test, y_test_hot),\n",
    "              callbacks=[reduce_lr, mcp_save],\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('out'):\n",
    "    os.mkdir('out')\n",
    "\n",
    "model = get_model()\n",
    "train(model, X_train, y_train_hot, X_test, y_test_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing MFCCs\n",
    "Here we loaded an audio file and classified it on the computer. This sped up MFCC debugging, even though it's pretty ugly to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave1, sr = librosa.load(\"cough.wav\", mono=True, sr=SAMPLE_RATE,duration=1.0)\n",
    "wave1\n",
    "print(wave1.shape)\n",
    "wave1[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe=wave1*32768\n",
    "observe[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(wave1)\n",
    "plt.show\n",
    "#wave = wave[::3]\n",
    "\n",
    "mfcc = librosa.feature.mfcc(wave1, sr=SAMPLE_RATE,S=None, n_mfcc=nmfcc,n_fft=n_fft, hop_length=hop_length,n_mels=nmels,fmax=22050)\n",
    "mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (20 > mfcc.shape[1]):\n",
    "        pad_width = 20 - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "else:\n",
    "        mfcc = mfcc[:, :20]\n",
    "mfcc[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(2)\n",
    "#plt.plot(mfcc)\n",
    "#plt.show        \n",
    "sample_reshaped = mfcc.reshape(1, nmfcc, nframe, channel)\n",
    "sample_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts one sample\n",
    "def predict(filepath, model):\n",
    "    sample = wav2mfcc(filepath)\n",
    "    sample\n",
    "    print(sample.shape)\n",
    "    sample_reshaped = sample.reshape(1, nmfcc,nframe, channel)\n",
    "    #sample_reshaped.shape\n",
    "    print(sample_reshaped.shape)\n",
    "    \n",
    "    return get_labels()[0][\n",
    "            np.argmax(model.predict(sample_reshaped))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_reshaped.shape\n",
    "print(sample_reshaped.shape)\n",
    "get_labels()[0][\n",
    "            np.argmax(model.predict(sample_reshaped))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to TF Lite\n",
    "**frozen_model_name:** The frozen model to convert. These are saved automatically in the `out/` directory during training.<br>\n",
    "**tf_lite_model_name:** The desired name of your TF model. We follow the following format: `dataset_feature_'i'inputsize_'l'labelsize`. So, a model trained on 6 office sounds with 1 second of amplitude at a sample rate of 44100 Hz would be named `OfficeSounds_Amplitude_i44100_l6.tflite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_model_name = 'out/weights-0.91.hdf5'\n",
    "tf_lite_model_name = \"amplitude-l6-acc91.tflite\"\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(frozen_model)\n",
    "tfmodel = converter.convert()\n",
    "open (tf_lite_name, \"wb\") .write(tfmodel)\n",
    "\n",
    "# filename = './out/frozen_Audio_Recorder.pb'\n",
    "# converter = tf.lite.TFLiteConverter.from_frozen_graph(filename,[\"voice_input\"], [\"label/Softmax\"])\n",
    "# tflite_model = converter.convert()\n",
    "# open(\"Yunus_OfficeSounds.tflite\", \"wb\").write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
